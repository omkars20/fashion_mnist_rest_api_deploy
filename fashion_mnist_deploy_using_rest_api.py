# -*- coding: utf-8 -*-
"""Assignment-9_Omkar_singh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11xB74OIIodlPmap-q_2LZUGizYNui-O1

# **Deploy a Fashion MNIST Model using TF Serving as a REST API**

**Install Aptitude Package >>>>> the default Debian package manager since Google's Colab runs in a Debian environment.**
"""

!echo "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \
curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -
!apt update

"""**Install the tensorflow model server**"""

!apt-get install tensorflow-model-server

"""**Import the Helper libraries**




"""

#Import tensorflow
import tensorflow as tf
# import keras
from tensorflow import keras
import numpy as np
# import matplotlib for image visulization
import matplotlib.pyplot as plt
import os
# import tempfile for temp file directory
import tempfile
# import json for create json object
import json
# import requests for sending post request
import requests
print('TensorFlow version: {}'.format(tf.__version__))

"""# **Build a Model on Fashion MNIST Dataset.**

**Import the Fashion MNIST Dataset.**
"""

fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# scale the values ( Normalization)
train_images = train_images / 255.0
test_images = test_images / 255.0

# reshape for feeding into the model
train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)
test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)

# Create a list of class names
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# print the shape of train and test dataset
print('\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))
print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))

"""**Create a sequential Model.**"""

model = keras.Sequential([
  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3,
                      strides=2, activation='relu', name='Conv1'),
  keras.layers.Flatten(),
  keras.layers.Dense(10, name='Dense')
])

"""**Print the model summary.**"""

model.summary()

"""**Compiling  the Model.**

using adam optimizer and loss as a sparseCategoricalCrossEntropy
"""

epochs = 10
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=[keras.metrics.SparseCategoricalAccuracy()])

"""**Fit the train dataset into Model.**"""

model.fit(train_images, train_labels, epochs=epochs,verbose=1)

"""**Evaluate the model on test dataset.**"""

test_loss, test_acc = model.evaluate(test_images, test_labels)
print('\nTest accuracy: {}'.format(test_acc))

"""# **Create export_path and save the Model on that.**"""

#model directory
MODEL_DIR = tempfile.gettempdir()

# version of model to be save
version = 1

# path to export
export_path = os.path.join(MODEL_DIR, str(version))

# if already present model in path than remove it and save again
if os.path.isdir(export_path):
    print('\nAlready saved a model, cleaning up\n')
    !rm -r {export_path}

# save the model
model.save(export_path, save_format="tf")

#print the export model path
print('\nexport_path = {}'.format(export_path))
!ls -l {export_path}

"""# **Examine the Saved Model.**"""

!saved_model_cli show --dir {export_path} --all

"""# **Run the TensorFlow Model Server**

**we will write the value of the Python variable to an environment variable using the os.environ function.**
"""

os.environ["MODEL_DIR"] = MODEL_DIR

"""**We will now launch the TensorFlow model server with a bash script.**"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash --bg
# nohup tensorflow_model_server \
#     # The port use for requests
#   --rest_api_port=8501 \
#     # Model name
#   --model_name=fashion_model \
#     # path to the directory where we saved your model.
#   --model_base_path="${MODEL_DIR}" >server.log 2>&1

"""**Now we can take a look at the server log.**"""

!tail server.log

"""**Create a show function to plot the Image.**"""

def show(idx, title):
  plt.figure()
  plt.imshow(test_images[idx].reshape(28,28))
  plt.axis('off')
  plt.title('\n\n{}'.format(title), fontdict={'size': 16})

"""**Create JSON Object with Test Data.**"""

data = json.dumps({"signature_name": "serving_default", "instances": test_images[0:3].tolist()})
print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))

"""# **Make Inference Request.**

Finally, we can make the inference request and get the inferences back. We'll send a predict request as a POST to our server's REST endpoint, and pass it our test data.

Error in connection request
"""

headers = {"content-type": "application/json"}
json_response = requests.post('http://localhost:8501/v1/models/fashion_model:predict', data=data, headers=headers)
predictions = json.loads(json_response.text)['predictions']

"""**Show the Predicted Image.**"""

show(0, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(
  class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[0]], test_labels[0]))